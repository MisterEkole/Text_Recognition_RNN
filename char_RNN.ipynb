{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":4,"language_info":{"name":"python","version":"3.8.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3.8.11 64-bit ('Ekole': conda)"},"interpreter":{"hash":"2e62bb52526f97eeac2ae89e1e99c4750580f5fc29d4337dcbaad0a5eccb5534"},"colab":{"name":"char_RNN.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"D2fYbk9YGS5D"},"source":["# Text Character recognition using RNN with Pytorch\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvr0S_CTGgpg","executionInfo":{"status":"ok","timestamp":1630319295425,"user_tz":-60,"elapsed":29612,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"861fd0c2-9f0a-467b-e32f-d0edc13b66e3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7v43iO5cGS5H","executionInfo":{"status":"ok","timestamp":1630321257904,"user_tz":-60,"elapsed":3,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#imports\n","\n","import torch\n","import torch as nn\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import numpy as np\n","import os"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3Fc5OspGS5J","executionInfo":{"status":"ok","timestamp":1630319512786,"user_tz":-60,"elapsed":1227,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#Loading the data\n","with open('/content/drive/MyDrive/Colab Notebooks/Char_RNN/data/anna.txt','r')as f:\n","    text_data= f.read()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"EXTo4RIuGS5K","executionInfo":{"status":"ok","timestamp":1630319515172,"user_tz":-60,"elapsed":6,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"b4ca19c5-dbfd-4a1c-b082-77268126a40d"},"source":["text_data[:150]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wi\""]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"GCRU5ZORGS5L"},"source":["Tokenisation, by converting each character to and from intergers\n"]},{"cell_type":"code","metadata":{"id":"8DDkLOPxGS5M","executionInfo":{"status":"ok","timestamp":1630319521077,"user_tz":-60,"elapsed":469,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["chars=tuple(set(text_data))\n","int2char=dict(enumerate(chars))\n","char2int={ch:ii for ii, ch in int2char.items()}\n","\n","#encode the text\n","encode_txt= np.array([char2int[ch] for ch in text_data])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tUk49IUYGS5N","executionInfo":{"status":"ok","timestamp":1630319524836,"user_tz":-60,"elapsed":377,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"d859a31c-3dd1-45fc-f264-aafdad26ca51"},"source":["encode_txt[:150]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([18, 58, 41, 50, 48,  2, 21, 65, 23, 16, 16, 16, 59, 41, 50, 50, 71,\n","       65, 70, 41, 61, 35,  0, 35,  2, 80, 65, 41, 21,  2, 65, 41,  0,  0,\n","       65, 41,  0, 35, 73,  2, 34, 65,  2, 17,  2, 21, 71, 65, 28, 22, 58,\n","       41, 50, 50, 71, 65, 70, 41, 61, 35,  0, 71, 65, 35, 80, 65, 28, 22,\n","       58, 41, 50, 50, 71, 65, 35, 22, 65, 35, 48, 80, 65, 43, 24, 22, 16,\n","       24, 41, 71, 68, 16, 16, 60, 17,  2, 21, 71, 48, 58, 35, 22, 82, 65,\n","       24, 41, 80, 65, 35, 22, 65, 55, 43, 22, 70, 28, 80, 35, 43, 22, 65,\n","       35, 22, 65, 48, 58,  2, 65, 30, 32,  0, 43, 22, 80, 73, 71, 80, 14,\n","       65, 58, 43, 28, 80,  2, 68, 65, 77, 58,  2, 65, 24, 35])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"5BNosoq3GS5O","executionInfo":{"status":"ok","timestamp":1630319529368,"user_tz":-60,"elapsed":363,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#data preprocessing\n","def one_hot_encode(arr, n_labels):\n","   #initialise encoded array\n","    one_hot=np.zeros((arr.size, n_labels), dtype=np.float32)\n","    #fill arrays with zeros\n","\n","    one_hot[np.arange(one_hot.shape[0]), arr.flatten()]=1.\n","    #reshaping array to original shape\n","    one_hot= one_hot.reshape((*arr.shape, n_labels))\n","\n","    return one_hot"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iXXQncQTGS5P","executionInfo":{"status":"ok","timestamp":1630319534230,"user_tz":-60,"elapsed":4,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"fee1a353-31ac-4029-b866-5e3a50f4544e"},"source":["#check if one_hot_encode works as expected\n","test= np.array([[3,5,1]])\n","one_hot=one_hot_encode(test,8)\n","print(one_hot)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[[[0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hhzr3dzxGS5Q","executionInfo":{"status":"ok","timestamp":1630319541643,"user_tz":-60,"elapsed":422,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#Creating a batch\n","#Creating a generator that returns batches of size= batch_size *seq_legth from arr\n","def batch_gen(arr, batch_size, seq_length):\n","    total_batch_size= batch_size*seq_length\n","    n_batches= len(arr)//total_batch_size\n","\n","    #reserve only enough characters to make batches full\n","    arr=arr[:n_batches*total_batch_size]\n","    #reshape array\n","    arr= arr.reshape((batch_size, -1))\n","    for n in range(0,arr.shape[1], seq_length):\n","        x=arr[:,n:n+seq_length]\n","\n","        y=np.zeros_like(x)\n","\n","        try:\n","             y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n","        except IndexError:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","        yield x, y\n","\n","\n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDBDgXqUGS5Q","executionInfo":{"status":"ok","timestamp":1630319557552,"user_tz":-60,"elapsed":392,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#testing batch generator\n","\n","batches=batch_gen(encode_txt, 8, 50)\n","x,y=next(batches)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"300zMDJKGS5Q","executionInfo":{"status":"ok","timestamp":1630319559028,"user_tz":-60,"elapsed":4,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"7fd61b07-79a7-4be2-c9ba-4d2631b514d3"},"source":["#printing first 10 items in sequence\n","\n","print('x\\n', x[:10, :10])\n","print('y\\n',y[:10,:10])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["x\n"," [[18 58 41 50 48  2 21 65 23 16]\n"," [80 43 22 65 48 58 41 48 65 41]\n"," [ 2 22 79 65 43 21 65 41 65 70]\n"," [80 65 48 58  2 65 55 58 35  2]\n"," [65 80 41 24 65 58  2 21 65 48]\n"," [55 28 80 80 35 43 22 65 41 22]\n"," [65 31 22 22 41 65 58 41 79 65]\n"," [30 32  0 43 22 80 73 71 68 65]]\n","y\n"," [[58 41 50 48  2 21 65 23 16 16]\n"," [43 22 65 48 58 41 48 65 41 48]\n"," [22 79 65 43 21 65 41 65 70 43]\n"," [65 48 58  2 65 55 58 35  2 70]\n"," [80 41 24 65 58  2 21 65 48  2]\n"," [28 80 80 35 43 22 65 41 22 79]\n"," [31 22 22 41 65 58 41 79 65 80]\n"," [32  0 43 22 80 73 71 68 65 76]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4hKVK1okGS5R"},"source":["## Defining the neural network with pytorch\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1oHHdvXGS5R","executionInfo":{"status":"ok","timestamp":1630319563799,"user_tz":-60,"elapsed":720,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"6743b714-dae5-4daa-9335-8acdae5ae9cd"},"source":["#checking for GPU\n","gpu_train= torch.cuda.is_available()\n","\n","if(gpu_train):\n","    print(\" Training on GPU\")\n","else:\n","    print(\"No GPU available\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":[" Training on GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E3iErkxnGS5R","executionInfo":{"status":"ok","timestamp":1630319574629,"user_tz":-60,"elapsed":5,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#Defining the RNN\n"," \n","class RNN(torch.nn.Module):\n","     n_hidden=256\n","     n_layers=2\n","     drop_prob=0.5\n","     lr=0.001\n","     def __init__(self, tokens, n_hidden,n_layers, drop_prob, lr):\n","            super().__init__()\n","            self.drop_prob=drop_prob\n","            self.n_layers=n_layers\n","            self.n_hidden=n_hidden\n","            self.lr=lr\n","\n","         #creating dict for characters\n","\n","            self.chars=tokens\n","            self.int2char=dict(enumerate(self.chars))\n","            self.char2int={ch: ii for ii, ch in self.int2char.items()}\n","\n","         #defining the LSTM\n","            self.lstm=torch.nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n","\n","         #dropout layer\n","\n","            self.dropout=torch.nn.Dropout(drop_prob)\n","\n","         #fully connected layer\n","            self.fc=torch.nn.Linear(n_hidden,len(self.chars))\n","            \n","     def forward(self, x, hidden):\n","                 #generate output and new hidden unit\n","                r_output, hidden=self.lstm(x, hidden)\n","\n","                #send output via drop out layer\n","\n","                out=self.dropout(r_output)\n","\n","                    #Stacking up LSTM\n","\n","                out=out.contiguous().view(-1, self.n_hidden)\n","\n","        #pass output through fully connected layer\n","\n","                out=self.fc(out)\n","                return out, hidden\n","     def init_hidden(self, batch_size):\n","        weight=next(self.parameters()).data\n","\n","        if(gpu_train):\n","            hidden=(weight.new(self.n_layers, batch_size,self.n_hidden).zero_().cuda(),\n","            weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n","        else:\n","            hidden=(weight.new(self.n_layers, batch_size,self.n_hidden).zero_(),\n","            weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","        return hidden\n","\n","\n","\n","\n","\n","\n","    "],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"5f1D8sOSGS5S","executionInfo":{"status":"ok","timestamp":1630319586511,"user_tz":-60,"elapsed":10,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["\n","#Defining the training loop\n","\n","def trainer(network, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, val_frac=0.1, print_every=10):\n","    network.train()\n","    optimiser= torch.optim.Adam(network.parameters(),lr=0.001)\n","    criterion= torch.nn.CrossEntropyLoss()\n","    \n","    #generating training and valid set\n","    \n","    val_idx= int(len(data)*(1-val_frac))\n","    data,val_data=data[:val_idx], data[val_idx:]\n","    \n","    if (gpu_train):\n","      \n","        network.cuda()\n","        counter=0\n","        n_chars=len(network.chars)\n","    for e in range(epochs):\n","        h=net.init_hidden(batch_size)\n","        for x, y in batch_gen(data, batch_size, seq_length):\n","            counter+=1\n","            \n","            x=one_hot_encode(x, n_chars)\n","            inputs, goal= torch.from_numpy(x), torch.from_numpy(y)\n","            if (gpu_train):\n","                inputs,goal= inputs.cuda(), goal.cuda()\n","                #new variable for hidden state or else backprop through whole channel\n","               \n","                h=tuple([each.data for each in h])\n","                \n","                #accumulating zero grads\n","                \n","                network.zero_grad()\n","                \n","                #generate model output\n","                \n","                output, h= network(inputs, h)\n","                \n","                #determine loss and do backprop\n","                loss=criterion(output, goal.view(batch_size*seq_length).long())\n","                \n","                loss.backward()\n","                \n","                #avoid exploding gradient by clipping\n","                clip=1.0\n","                torch.nn.utils.clip_grad_norm(network.parameters(), clip)\n","                optimiser.step()\n","                \n","                #loss stats\n","                \n","                if counter %print_every==0:\n","                    val_h= network.init_hidden(batch_size)\n","                    \n","                    val_losses=[]\n","                    network.eval()\n","                    \n","                    for x, y in batch_gen(val_data, batch_size, seq_length):\n","                        x=one_hot_encode(x, n_chars)\n","                        x,y=torch.from_numpy(x), torch.from_numpy(y)\n","                        \n","                        #new val for val hidden state and backprop through whole channe\n","                        val_h= tuple([each.data for each in val_h])\n","                        \n","                        inputs, goal=x,y\n","                        if(gpu_train):\n","                            inputs, goal=inputs.cuda(), goal.cuda()\n","                            \n","                        output, val_h= network(inputs, val_h)\n","                        val_loss= criterion(output, goal.view(batch_size*seq_length).long())\n","                        val_losses.append(val_loss.item())\n","                    network.train()\n","                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                          \"Steps: {}...\".format(counter),\n","                          \"Loss: {:.4f}...\".format(loss.item()),\n","                          \"val_loss:  {:4f}\".format(np.mean(val_losses)))\n","\n","\n","\n","                    "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHB_gmrUGS5S","executionInfo":{"status":"ok","timestamp":1630319599425,"user_tz":-60,"elapsed":5740,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"b00ba08e-ce50-4536-ef21-5c7e7bc0c713"},"source":["#instantiating the model\n","\n","n_hidden= 512\n","n_layers= 2\n","lr=0.001\n","drop_prob=0.5\n","net=RNN(chars,n_hidden, n_layers, drop_prob, lr)\n","\n","print(net)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["RNN(\n","  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=512, out_features=83, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_2E_HOSGS5T","executionInfo":{"status":"ok","timestamp":1630320610308,"user_tz":-60,"elapsed":161493,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"dffd3f72-64fb-4dfb-90c5-15058f47261f"},"source":["batch_size= 128\n","seq_length=100\n","n_epochs=50\n","\n","#train the model\n","trainer(net, encode_txt, epochs=n_epochs, seq_length=seq_length, batch_size=batch_size)"],"execution_count":18,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"]},{"output_type":"stream","text":["Epoch: 1/50... Steps: 10... Loss: 2.2276... val_loss:  2.193624\n","Epoch: 1/50... Steps: 20... Loss: 2.1783... val_loss:  2.155711\n","Epoch: 1/50... Steps: 30... Loss: 2.1710... val_loss:  2.129650\n","Epoch: 1/50... Steps: 40... Loss: 2.1298... val_loss:  2.110672\n","Epoch: 1/50... Steps: 50... Loss: 2.1504... val_loss:  2.092978\n","Epoch: 1/50... Steps: 60... Loss: 2.0757... val_loss:  2.078940\n","Epoch: 1/50... Steps: 70... Loss: 2.0830... val_loss:  2.064860\n","Epoch: 1/50... Steps: 80... Loss: 2.0558... val_loss:  2.048202\n","Epoch: 1/50... Steps: 90... Loss: 2.0725... val_loss:  2.030150\n","Epoch: 1/50... Steps: 100... Loss: 2.0366... val_loss:  2.015803\n","Epoch: 1/50... Steps: 110... Loss: 2.0219... val_loss:  1.999008\n","Epoch: 1/50... Steps: 120... Loss: 1.9669... val_loss:  1.978324\n","Epoch: 1/50... Steps: 130... Loss: 2.0084... val_loss:  1.966215\n","Epoch: 2/50... Steps: 140... Loss: 2.0020... val_loss:  1.949462\n","Epoch: 2/50... Steps: 150... Loss: 1.9762... val_loss:  1.939422\n","Epoch: 2/50... Steps: 160... Loss: 1.9708... val_loss:  1.920622\n","Epoch: 2/50... Steps: 170... Loss: 1.9396... val_loss:  1.907007\n","Epoch: 2/50... Steps: 180... Loss: 1.8986... val_loss:  1.891801\n","Epoch: 2/50... Steps: 190... Loss: 1.8572... val_loss:  1.875627\n","Epoch: 2/50... Steps: 200... Loss: 1.8675... val_loss:  1.862719\n","Epoch: 2/50... Steps: 210... Loss: 1.8664... val_loss:  1.846718\n","Epoch: 2/50... Steps: 220... Loss: 1.8349... val_loss:  1.834194\n","Epoch: 2/50... Steps: 230... Loss: 1.8488... val_loss:  1.818944\n","Epoch: 2/50... Steps: 240... Loss: 1.8422... val_loss:  1.805763\n","Epoch: 2/50... Steps: 250... Loss: 1.7998... val_loss:  1.793278\n","Epoch: 2/50... Steps: 260... Loss: 1.7616... val_loss:  1.779988\n","Epoch: 2/50... Steps: 270... Loss: 1.7983... val_loss:  1.767589\n","Epoch: 3/50... Steps: 280... Loss: 1.7800... val_loss:  1.756220\n","Epoch: 3/50... Steps: 290... Loss: 1.7851... val_loss:  1.749543\n","Epoch: 3/50... Steps: 300... Loss: 1.7445... val_loss:  1.731534\n","Epoch: 3/50... Steps: 310... Loss: 1.7365... val_loss:  1.724689\n","Epoch: 3/50... Steps: 320... Loss: 1.7056... val_loss:  1.710721\n","Epoch: 3/50... Steps: 330... Loss: 1.7053... val_loss:  1.702781\n","Epoch: 3/50... Steps: 340... Loss: 1.7541... val_loss:  1.693231\n","Epoch: 3/50... Steps: 350... Loss: 1.6910... val_loss:  1.686117\n","Epoch: 3/50... Steps: 360... Loss: 1.6543... val_loss:  1.678327\n","Epoch: 3/50... Steps: 370... Loss: 1.6832... val_loss:  1.670247\n","Epoch: 3/50... Steps: 380... Loss: 1.6778... val_loss:  1.660506\n","Epoch: 3/50... Steps: 390... Loss: 1.6583... val_loss:  1.652228\n","Epoch: 3/50... Steps: 400... Loss: 1.6408... val_loss:  1.643710\n","Epoch: 3/50... Steps: 410... Loss: 1.6552... val_loss:  1.635818\n","Epoch: 4/50... Steps: 420... Loss: 1.6504... val_loss:  1.636698\n","Epoch: 4/50... Steps: 430... Loss: 1.6450... val_loss:  1.623619\n","Epoch: 4/50... Steps: 440... Loss: 1.6339... val_loss:  1.616326\n","Epoch: 4/50... Steps: 450... Loss: 1.5895... val_loss:  1.605415\n","Epoch: 4/50... Steps: 460... Loss: 1.5645... val_loss:  1.601928\n","Epoch: 4/50... Steps: 470... Loss: 1.6283... val_loss:  1.594805\n","Epoch: 4/50... Steps: 480... Loss: 1.6073... val_loss:  1.588756\n","Epoch: 4/50... Steps: 490... Loss: 1.6092... val_loss:  1.583018\n","Epoch: 4/50... Steps: 500... Loss: 1.6109... val_loss:  1.575442\n","Epoch: 4/50... Steps: 510... Loss: 1.5847... val_loss:  1.573004\n","Epoch: 4/50... Steps: 520... Loss: 1.5953... val_loss:  1.564166\n","Epoch: 4/50... Steps: 530... Loss: 1.5629... val_loss:  1.563495\n","Epoch: 4/50... Steps: 540... Loss: 1.5278... val_loss:  1.554408\n","Epoch: 4/50... Steps: 550... Loss: 1.5912... val_loss:  1.551429\n","Epoch: 5/50... Steps: 560... Loss: 1.5504... val_loss:  1.545179\n","Epoch: 5/50... Steps: 570... Loss: 1.5526... val_loss:  1.539550\n","Epoch: 5/50... Steps: 580... Loss: 1.5296... val_loss:  1.534734\n","Epoch: 5/50... Steps: 590... Loss: 1.5272... val_loss:  1.529267\n","Epoch: 5/50... Steps: 600... Loss: 1.5200... val_loss:  1.525275\n","Epoch: 5/50... Steps: 610... Loss: 1.4997... val_loss:  1.518189\n","Epoch: 5/50... Steps: 620... Loss: 1.5186... val_loss:  1.515597\n","Epoch: 5/50... Steps: 630... Loss: 1.5366... val_loss:  1.518527\n","Epoch: 5/50... Steps: 640... Loss: 1.5018... val_loss:  1.506660\n","Epoch: 5/50... Steps: 650... Loss: 1.5021... val_loss:  1.502942\n","Epoch: 5/50... Steps: 660... Loss: 1.4892... val_loss:  1.501261\n","Epoch: 5/50... Steps: 670... Loss: 1.5187... val_loss:  1.500636\n","Epoch: 5/50... Steps: 680... Loss: 1.5016... val_loss:  1.490463\n","Epoch: 5/50... Steps: 690... Loss: 1.4848... val_loss:  1.492329\n","Epoch: 6/50... Steps: 700... Loss: 1.4917... val_loss:  1.486426\n","Epoch: 6/50... Steps: 710... Loss: 1.4722... val_loss:  1.486118\n","Epoch: 6/50... Steps: 720... Loss: 1.4695... val_loss:  1.481295\n","Epoch: 6/50... Steps: 730... Loss: 1.4873... val_loss:  1.479913\n","Epoch: 6/50... Steps: 740... Loss: 1.4522... val_loss:  1.472081\n","Epoch: 6/50... Steps: 750... Loss: 1.4494... val_loss:  1.466919\n","Epoch: 6/50... Steps: 760... Loss: 1.4826... val_loss:  1.467997\n","Epoch: 6/50... Steps: 770... Loss: 1.4489... val_loss:  1.471832\n","Epoch: 6/50... Steps: 780... Loss: 1.4518... val_loss:  1.458170\n","Epoch: 6/50... Steps: 790... Loss: 1.4387... val_loss:  1.456516\n","Epoch: 6/50... Steps: 800... Loss: 1.4601... val_loss:  1.455836\n","Epoch: 6/50... Steps: 810... Loss: 1.4430... val_loss:  1.453052\n","Epoch: 6/50... Steps: 820... Loss: 1.4081... val_loss:  1.450056\n","Epoch: 6/50... Steps: 830... Loss: 1.4459... val_loss:  1.446011\n","Epoch: 7/50... Steps: 840... Loss: 1.4176... val_loss:  1.441864\n","Epoch: 7/50... Steps: 850... Loss: 1.4307... val_loss:  1.438769\n","Epoch: 7/50... Steps: 860... Loss: 1.4102... val_loss:  1.436339\n","Epoch: 7/50... Steps: 870... Loss: 1.4200... val_loss:  1.434793\n","Epoch: 7/50... Steps: 880... Loss: 1.4161... val_loss:  1.431857\n","Epoch: 7/50... Steps: 890... Loss: 1.4258... val_loss:  1.428395\n","Epoch: 7/50... Steps: 900... Loss: 1.4181... val_loss:  1.430615\n","Epoch: 7/50... Steps: 910... Loss: 1.3905... val_loss:  1.428955\n","Epoch: 7/50... Steps: 920... Loss: 1.4027... val_loss:  1.423124\n","Epoch: 7/50... Steps: 930... Loss: 1.3975... val_loss:  1.418676\n","Epoch: 7/50... Steps: 940... Loss: 1.3920... val_loss:  1.421994\n","Epoch: 7/50... Steps: 950... Loss: 1.4183... val_loss:  1.418903\n","Epoch: 7/50... Steps: 960... Loss: 1.4192... val_loss:  1.414953\n","Epoch: 7/50... Steps: 970... Loss: 1.4207... val_loss:  1.411219\n","Epoch: 8/50... Steps: 980... Loss: 1.3857... val_loss:  1.410903\n","Epoch: 8/50... Steps: 990... Loss: 1.3865... val_loss:  1.406547\n","Epoch: 8/50... Steps: 1000... Loss: 1.3874... val_loss:  1.404188\n","Epoch: 8/50... Steps: 1010... Loss: 1.4180... val_loss:  1.404297\n","Epoch: 8/50... Steps: 1020... Loss: 1.3981... val_loss:  1.403706\n","Epoch: 8/50... Steps: 1030... Loss: 1.3848... val_loss:  1.400097\n","Epoch: 8/50... Steps: 1040... Loss: 1.3846... val_loss:  1.400142\n","Epoch: 8/50... Steps: 1050... Loss: 1.3642... val_loss:  1.402568\n","Epoch: 8/50... Steps: 1060... Loss: 1.3715... val_loss:  1.393909\n","Epoch: 8/50... Steps: 1070... Loss: 1.3893... val_loss:  1.391364\n","Epoch: 8/50... Steps: 1080... Loss: 1.3756... val_loss:  1.399057\n","Epoch: 8/50... Steps: 1090... Loss: 1.3640... val_loss:  1.391700\n","Epoch: 8/50... Steps: 1100... Loss: 1.3517... val_loss:  1.389827\n","Epoch: 8/50... Steps: 1110... Loss: 1.3656... val_loss:  1.385744\n","Epoch: 9/50... Steps: 1120... Loss: 1.3808... val_loss:  1.384621\n","Epoch: 9/50... Steps: 1130... Loss: 1.3784... val_loss:  1.384414\n","Epoch: 9/50... Steps: 1140... Loss: 1.3845... val_loss:  1.381559\n","Epoch: 9/50... Steps: 1150... Loss: 1.3950... val_loss:  1.382149\n","Epoch: 9/50... Steps: 1160... Loss: 1.3474... val_loss:  1.380597\n","Epoch: 9/50... Steps: 1170... Loss: 1.3601... val_loss:  1.376801\n","Epoch: 9/50... Steps: 1180... Loss: 1.3478... val_loss:  1.375224\n","Epoch: 9/50... Steps: 1190... Loss: 1.3849... val_loss:  1.376278\n","Epoch: 9/50... Steps: 1200... Loss: 1.3305... val_loss:  1.373237\n","Epoch: 9/50... Steps: 1210... Loss: 1.3377... val_loss:  1.372201\n","Epoch: 9/50... Steps: 1220... Loss: 1.3370... val_loss:  1.372862\n","Epoch: 9/50... Steps: 1230... Loss: 1.3242... val_loss:  1.369423\n","Epoch: 9/50... Steps: 1240... Loss: 1.3345... val_loss:  1.364829\n","Epoch: 9/50... Steps: 1250... Loss: 1.3474... val_loss:  1.368146\n","Epoch: 10/50... Steps: 1260... Loss: 1.3430... val_loss:  1.369728\n","Epoch: 10/50... Steps: 1270... Loss: 1.3465... val_loss:  1.365455\n","Epoch: 10/50... Steps: 1280... Loss: 1.3611... val_loss:  1.362080\n","Epoch: 10/50... Steps: 1290... Loss: 1.3546... val_loss:  1.366096\n","Epoch: 10/50... Steps: 1300... Loss: 1.3377... val_loss:  1.361936\n","Epoch: 10/50... Steps: 1310... Loss: 1.3408... val_loss:  1.361916\n","Epoch: 10/50... Steps: 1320... Loss: 1.3144... val_loss:  1.361081\n","Epoch: 10/50... Steps: 1330... Loss: 1.3186... val_loss:  1.359366\n","Epoch: 10/50... Steps: 1340... Loss: 1.3077... val_loss:  1.358357\n","Epoch: 10/50... Steps: 1350... Loss: 1.3101... val_loss:  1.353170\n","Epoch: 10/50... Steps: 1360... Loss: 1.3053... val_loss:  1.358439\n","Epoch: 10/50... Steps: 1370... Loss: 1.2960... val_loss:  1.354728\n","Epoch: 10/50... Steps: 1380... Loss: 1.3371... val_loss:  1.352534\n","Epoch: 10/50... Steps: 1390... Loss: 1.3528... val_loss:  1.350663\n","Epoch: 11/50... Steps: 1400... Loss: 1.3526... val_loss:  1.349199\n","Epoch: 11/50... Steps: 1410... Loss: 1.3723... val_loss:  1.348641\n","Epoch: 11/50... Steps: 1420... Loss: 1.3475... val_loss:  1.345979\n","Epoch: 11/50... Steps: 1430... Loss: 1.3110... val_loss:  1.347211\n","Epoch: 11/50... Steps: 1440... Loss: 1.3435... val_loss:  1.347682\n","Epoch: 11/50... Steps: 1450... Loss: 1.2748... val_loss:  1.348852\n","Epoch: 11/50... Steps: 1460... Loss: 1.3007... val_loss:  1.345217\n","Epoch: 11/50... Steps: 1470... Loss: 1.2973... val_loss:  1.347339\n","Epoch: 11/50... Steps: 1480... Loss: 1.3160... val_loss:  1.339944\n","Epoch: 11/50... Steps: 1490... Loss: 1.3049... val_loss:  1.344083\n","Epoch: 11/50... Steps: 1500... Loss: 1.2959... val_loss:  1.343568\n","Epoch: 11/50... Steps: 1510... Loss: 1.2724... val_loss:  1.339000\n","Epoch: 11/50... Steps: 1520... Loss: 1.3082... val_loss:  1.337949\n","Epoch: 12/50... Steps: 1530... Loss: 1.3583... val_loss:  1.343186\n","Epoch: 12/50... Steps: 1540... Loss: 1.3220... val_loss:  1.338197\n","Epoch: 12/50... Steps: 1550... Loss: 1.3179... val_loss:  1.336455\n","Epoch: 12/50... Steps: 1560... Loss: 1.3384... val_loss:  1.333689\n","Epoch: 12/50... Steps: 1570... Loss: 1.2849... val_loss:  1.334122\n","Epoch: 12/50... Steps: 1580... Loss: 1.2686... val_loss:  1.335083\n","Epoch: 12/50... Steps: 1590... Loss: 1.2590... val_loss:  1.333482\n","Epoch: 12/50... Steps: 1600... Loss: 1.2813... val_loss:  1.331459\n","Epoch: 12/50... Steps: 1610... Loss: 1.2847... val_loss:  1.333585\n","Epoch: 12/50... Steps: 1620... Loss: 1.2719... val_loss:  1.329987\n","Epoch: 12/50... Steps: 1630... Loss: 1.3052... val_loss:  1.331376\n","Epoch: 12/50... Steps: 1640... Loss: 1.2780... val_loss:  1.329693\n","Epoch: 12/50... Steps: 1650... Loss: 1.2579... val_loss:  1.328221\n","Epoch: 12/50... Steps: 1660... Loss: 1.3151... val_loss:  1.328482\n","Epoch: 13/50... Steps: 1670... Loss: 1.2955... val_loss:  1.336177\n","Epoch: 13/50... Steps: 1680... Loss: 1.2921... val_loss:  1.327763\n","Epoch: 13/50... Steps: 1690... Loss: 1.2848... val_loss:  1.322950\n","Epoch: 13/50... Steps: 1700... Loss: 1.2803... val_loss:  1.319136\n","Epoch: 13/50... Steps: 1710... Loss: 1.2497... val_loss:  1.322780\n","Epoch: 13/50... Steps: 1720... Loss: 1.2668... val_loss:  1.322055\n","Epoch: 13/50... Steps: 1730... Loss: 1.3104... val_loss:  1.325310\n","Epoch: 13/50... Steps: 1740... Loss: 1.2684... val_loss:  1.322334\n","Epoch: 13/50... Steps: 1750... Loss: 1.2395... val_loss:  1.321690\n","Epoch: 13/50... Steps: 1760... Loss: 1.2570... val_loss:  1.323307\n","Epoch: 13/50... Steps: 1770... Loss: 1.2854... val_loss:  1.318929\n","Epoch: 13/50... Steps: 1780... Loss: 1.2590... val_loss:  1.319350\n","Epoch: 13/50... Steps: 1790... Loss: 1.2629... val_loss:  1.319566\n","Epoch: 13/50... Steps: 1800... Loss: 1.2832... val_loss:  1.315332\n","Epoch: 14/50... Steps: 1810... Loss: 1.2861... val_loss:  1.324818\n","Epoch: 14/50... Steps: 1820... Loss: 1.2671... val_loss:  1.317874\n","Epoch: 14/50... Steps: 1830... Loss: 1.2904... val_loss:  1.315536\n","Epoch: 14/50... Steps: 1840... Loss: 1.2343... val_loss:  1.316650\n","Epoch: 14/50... Steps: 1850... Loss: 1.2191... val_loss:  1.316514\n","Epoch: 14/50... Steps: 1860... Loss: 1.2708... val_loss:  1.316242\n","Epoch: 14/50... Steps: 1870... Loss: 1.2869... val_loss:  1.313853\n","Epoch: 14/50... Steps: 1880... Loss: 1.2699... val_loss:  1.315397\n","Epoch: 14/50... Steps: 1890... Loss: 1.2863... val_loss:  1.315533\n","Epoch: 14/50... Steps: 1900... Loss: 1.2690... val_loss:  1.311915\n","Epoch: 14/50... Steps: 1910... Loss: 1.2679... val_loss:  1.313102\n","Epoch: 14/50... Steps: 1920... Loss: 1.2614... val_loss:  1.310731\n","Epoch: 14/50... Steps: 1930... Loss: 1.2276... val_loss:  1.312128\n","Epoch: 14/50... Steps: 1940... Loss: 1.2834... val_loss:  1.310175\n","Epoch: 15/50... Steps: 1950... Loss: 1.2556... val_loss:  1.311728\n","Epoch: 15/50... Steps: 1960... Loss: 1.2633... val_loss:  1.313414\n","Epoch: 15/50... Steps: 1970... Loss: 1.2468... val_loss:  1.309989\n","Epoch: 15/50... Steps: 1980... Loss: 1.2460... val_loss:  1.312415\n","Epoch: 15/50... Steps: 1990... Loss: 1.2381... val_loss:  1.309643\n","Epoch: 15/50... Steps: 2000... Loss: 1.2280... val_loss:  1.306660\n","Epoch: 15/50... Steps: 2010... Loss: 1.2489... val_loss:  1.307052\n","Epoch: 15/50... Steps: 2020... Loss: 1.2696... val_loss:  1.306760\n","Epoch: 15/50... Steps: 2030... Loss: 1.2430... val_loss:  1.309263\n","Epoch: 15/50... Steps: 2040... Loss: 1.2522... val_loss:  1.307543\n","Epoch: 15/50... Steps: 2050... Loss: 1.2388... val_loss:  1.304509\n","Epoch: 15/50... Steps: 2060... Loss: 1.2436... val_loss:  1.305619\n","Epoch: 15/50... Steps: 2070... Loss: 1.2576... val_loss:  1.303044\n","Epoch: 15/50... Steps: 2080... Loss: 1.2547... val_loss:  1.304179\n","Epoch: 16/50... Steps: 2090... Loss: 1.2557... val_loss:  1.305758\n","Epoch: 16/50... Steps: 2100... Loss: 1.2348... val_loss:  1.304511\n","Epoch: 16/50... Steps: 2110... Loss: 1.2320... val_loss:  1.302736\n","Epoch: 16/50... Steps: 2120... Loss: 1.2489... val_loss:  1.301226\n","Epoch: 16/50... Steps: 2130... Loss: 1.2183... val_loss:  1.303408\n","Epoch: 16/50... Steps: 2140... Loss: 1.2384... val_loss:  1.302858\n","Epoch: 16/50... Steps: 2150... Loss: 1.2584... val_loss:  1.298985\n","Epoch: 16/50... Steps: 2160... Loss: 1.2381... val_loss:  1.300526\n","Epoch: 16/50... Steps: 2170... Loss: 1.2370... val_loss:  1.299959\n","Epoch: 16/50... Steps: 2180... Loss: 1.2313... val_loss:  1.298550\n","Epoch: 16/50... Steps: 2190... Loss: 1.2508... val_loss:  1.296179\n","Epoch: 16/50... Steps: 2200... Loss: 1.2274... val_loss:  1.298027\n","Epoch: 16/50... Steps: 2210... Loss: 1.1938... val_loss:  1.299789\n","Epoch: 16/50... Steps: 2220... Loss: 1.2278... val_loss:  1.296631\n","Epoch: 17/50... Steps: 2230... Loss: 1.2228... val_loss:  1.300065\n","Epoch: 17/50... Steps: 2240... Loss: 1.2268... val_loss:  1.300329\n","Epoch: 17/50... Steps: 2250... Loss: 1.2143... val_loss:  1.298145\n","Epoch: 17/50... Steps: 2260... Loss: 1.2253... val_loss:  1.300474\n","Epoch: 17/50... Steps: 2270... Loss: 1.2278... val_loss:  1.295771\n","Epoch: 17/50... Steps: 2280... Loss: 1.2365... val_loss:  1.294945\n","Epoch: 17/50... Steps: 2290... Loss: 1.2400... val_loss:  1.294282\n","Epoch: 17/50... Steps: 2300... Loss: 1.2033... val_loss:  1.299032\n","Epoch: 17/50... Steps: 2310... Loss: 1.2215... val_loss:  1.297462\n","Epoch: 17/50... Steps: 2320... Loss: 1.2179... val_loss:  1.296480\n","Epoch: 17/50... Steps: 2330... Loss: 1.2219... val_loss:  1.295370\n","Epoch: 17/50... Steps: 2340... Loss: 1.2333... val_loss:  1.295234\n","Epoch: 17/50... Steps: 2350... Loss: 1.2300... val_loss:  1.292434\n","Epoch: 17/50... Steps: 2360... Loss: 1.2415... val_loss:  1.295531\n","Epoch: 18/50... Steps: 2370... Loss: 1.2083... val_loss:  1.294160\n","Epoch: 18/50... Steps: 2380... Loss: 1.2121... val_loss:  1.292349\n","Epoch: 18/50... Steps: 2390... Loss: 1.2240... val_loss:  1.293230\n","Epoch: 18/50... Steps: 2400... Loss: 1.2343... val_loss:  1.293738\n","Epoch: 18/50... Steps: 2410... Loss: 1.2391... val_loss:  1.290695\n","Epoch: 18/50... Steps: 2420... Loss: 1.2177... val_loss:  1.288930\n","Epoch: 18/50... Steps: 2430... Loss: 1.2307... val_loss:  1.286525\n","Epoch: 18/50... Steps: 2440... Loss: 1.2110... val_loss:  1.289968\n","Epoch: 18/50... Steps: 2450... Loss: 1.2066... val_loss:  1.291192\n","Epoch: 18/50... Steps: 2460... Loss: 1.2255... val_loss:  1.287454\n","Epoch: 18/50... Steps: 2470... Loss: 1.2131... val_loss:  1.287285\n","Epoch: 18/50... Steps: 2480... Loss: 1.2096... val_loss:  1.287872\n","Epoch: 18/50... Steps: 2490... Loss: 1.1947... val_loss:  1.287234\n","Epoch: 18/50... Steps: 2500... Loss: 1.2027... val_loss:  1.287275\n","Epoch: 19/50... Steps: 2510... Loss: 1.2125... val_loss:  1.286859\n","Epoch: 19/50... Steps: 2520... Loss: 1.2311... val_loss:  1.289556\n","Epoch: 19/50... Steps: 2530... Loss: 1.2327... val_loss:  1.289499\n","Epoch: 19/50... Steps: 2540... Loss: 1.2376... val_loss:  1.286703\n","Epoch: 19/50... Steps: 2550... Loss: 1.2017... val_loss:  1.286800\n","Epoch: 19/50... Steps: 2560... Loss: 1.2235... val_loss:  1.284714\n","Epoch: 19/50... Steps: 2570... Loss: 1.2063... val_loss:  1.286668\n","Epoch: 19/50... Steps: 2580... Loss: 1.2359... val_loss:  1.289064\n","Epoch: 19/50... Steps: 2590... Loss: 1.1928... val_loss:  1.285825\n","Epoch: 19/50... Steps: 2600... Loss: 1.1986... val_loss:  1.284198\n","Epoch: 19/50... Steps: 2610... Loss: 1.1974... val_loss:  1.286673\n","Epoch: 19/50... Steps: 2620... Loss: 1.1913... val_loss:  1.284114\n","Epoch: 19/50... Steps: 2630... Loss: 1.1921... val_loss:  1.284244\n","Epoch: 19/50... Steps: 2640... Loss: 1.2121... val_loss:  1.286981\n","Epoch: 20/50... Steps: 2650... Loss: 1.2090... val_loss:  1.284898\n","Epoch: 20/50... Steps: 2660... Loss: 1.2095... val_loss:  1.284947\n","Epoch: 20/50... Steps: 2670... Loss: 1.2251... val_loss:  1.286267\n","Epoch: 20/50... Steps: 2680... Loss: 1.2156... val_loss:  1.283354\n","Epoch: 20/50... Steps: 2690... Loss: 1.2041... val_loss:  1.281009\n","Epoch: 20/50... Steps: 2700... Loss: 1.2125... val_loss:  1.280335\n","Epoch: 20/50... Steps: 2710... Loss: 1.1800... val_loss:  1.280260\n","Epoch: 20/50... Steps: 2720... Loss: 1.1757... val_loss:  1.281572\n","Epoch: 20/50... Steps: 2730... Loss: 1.1771... val_loss:  1.280219\n","Epoch: 20/50... Steps: 2740... Loss: 1.1879... val_loss:  1.282001\n","Epoch: 20/50... Steps: 2750... Loss: 1.1942... val_loss:  1.280307\n","Epoch: 20/50... Steps: 2760... Loss: 1.1844... val_loss:  1.281581\n","Epoch: 20/50... Steps: 2770... Loss: 1.2205... val_loss:  1.275159\n","Epoch: 20/50... Steps: 2780... Loss: 1.2406... val_loss:  1.277464\n","Epoch: 21/50... Steps: 2790... Loss: 1.2300... val_loss:  1.279560\n","Epoch: 21/50... Steps: 2800... Loss: 1.2372... val_loss:  1.278393\n","Epoch: 21/50... Steps: 2810... Loss: 1.2327... val_loss:  1.279944\n","Epoch: 21/50... Steps: 2820... Loss: 1.2019... val_loss:  1.276469\n","Epoch: 21/50... Steps: 2830... Loss: 1.2196... val_loss:  1.271734\n","Epoch: 21/50... Steps: 2840... Loss: 1.1636... val_loss:  1.275766\n","Epoch: 21/50... Steps: 2850... Loss: 1.1895... val_loss:  1.279383\n","Epoch: 21/50... Steps: 2860... Loss: 1.1697... val_loss:  1.278622\n","Epoch: 21/50... Steps: 2870... Loss: 1.1989... val_loss:  1.279028\n","Epoch: 21/50... Steps: 2880... Loss: 1.1875... val_loss:  1.278490\n","Epoch: 21/50... Steps: 2890... Loss: 1.1809... val_loss:  1.277669\n","Epoch: 21/50... Steps: 2900... Loss: 1.1713... val_loss:  1.276401\n","Epoch: 21/50... Steps: 2910... Loss: 1.1970... val_loss:  1.275762\n","Epoch: 22/50... Steps: 2920... Loss: 1.2412... val_loss:  1.276514\n","Epoch: 22/50... Steps: 2930... Loss: 1.2082... val_loss:  1.275889\n","Epoch: 22/50... Steps: 2940... Loss: 1.2062... val_loss:  1.273539\n","Epoch: 22/50... Steps: 2950... Loss: 1.2174... val_loss:  1.276291\n","Epoch: 22/50... Steps: 2960... Loss: 1.1770... val_loss:  1.273688\n","Epoch: 22/50... Steps: 2970... Loss: 1.1636... val_loss:  1.271181\n","Epoch: 22/50... Steps: 2980... Loss: 1.1591... val_loss:  1.272547\n","Epoch: 22/50... Steps: 2990... Loss: 1.1719... val_loss:  1.275018\n","Epoch: 22/50... Steps: 3000... Loss: 1.1725... val_loss:  1.274814\n","Epoch: 22/50... Steps: 3010... Loss: 1.1815... val_loss:  1.271179\n","Epoch: 22/50... Steps: 3020... Loss: 1.1933... val_loss:  1.274120\n","Epoch: 22/50... Steps: 3030... Loss: 1.1698... val_loss:  1.274645\n","Epoch: 22/50... Steps: 3040... Loss: 1.1538... val_loss:  1.273389\n","Epoch: 22/50... Steps: 3050... Loss: 1.2067... val_loss:  1.275485\n","Epoch: 23/50... Steps: 3060... Loss: 1.1818... val_loss:  1.278902\n","Epoch: 23/50... Steps: 3070... Loss: 1.1862... val_loss:  1.276844\n","Epoch: 23/50... Steps: 3080... Loss: 1.1831... val_loss:  1.275448\n","Epoch: 23/50... Steps: 3090... Loss: 1.1788... val_loss:  1.274362\n","Epoch: 23/50... Steps: 3100... Loss: 1.1571... val_loss:  1.270712\n","Epoch: 23/50... Steps: 3110... Loss: 1.1697... val_loss:  1.273390\n","Epoch: 23/50... Steps: 3120... Loss: 1.2018... val_loss:  1.275330\n","Epoch: 23/50... Steps: 3130... Loss: 1.1696... val_loss:  1.277200\n","Epoch: 23/50... Steps: 3140... Loss: 1.1352... val_loss:  1.277042\n","Epoch: 23/50... Steps: 3150... Loss: 1.1635... val_loss:  1.274326\n","Epoch: 23/50... Steps: 3160... Loss: 1.1923... val_loss:  1.272155\n","Epoch: 23/50... Steps: 3170... Loss: 1.1652... val_loss:  1.275818\n","Epoch: 23/50... Steps: 3180... Loss: 1.1664... val_loss:  1.275638\n","Epoch: 23/50... Steps: 3190... Loss: 1.1748... val_loss:  1.269356\n","Epoch: 24/50... Steps: 3200... Loss: 1.1940... val_loss:  1.271005\n","Epoch: 24/50... Steps: 3210... Loss: 1.1682... val_loss:  1.272554\n","Epoch: 24/50... Steps: 3220... Loss: 1.1936... val_loss:  1.268716\n","Epoch: 24/50... Steps: 3230... Loss: 1.1483... val_loss:  1.269777\n","Epoch: 24/50... Steps: 3240... Loss: 1.1259... val_loss:  1.268326\n","Epoch: 24/50... Steps: 3250... Loss: 1.1833... val_loss:  1.269732\n","Epoch: 24/50... Steps: 3260... Loss: 1.1883... val_loss:  1.269965\n","Epoch: 24/50... Steps: 3270... Loss: 1.1760... val_loss:  1.271050\n","Epoch: 24/50... Steps: 3280... Loss: 1.1939... val_loss:  1.270345\n","Epoch: 24/50... Steps: 3290... Loss: 1.1750... val_loss:  1.272953\n","Epoch: 24/50... Steps: 3300... Loss: 1.1687... val_loss:  1.271168\n","Epoch: 24/50... Steps: 3310... Loss: 1.1749... val_loss:  1.271236\n","Epoch: 24/50... Steps: 3320... Loss: 1.1454... val_loss:  1.272602\n","Epoch: 24/50... Steps: 3330... Loss: 1.1927... val_loss:  1.266236\n","Epoch: 25/50... Steps: 3340... Loss: 1.1596... val_loss:  1.273476\n","Epoch: 25/50... Steps: 3350... Loss: 1.1675... val_loss:  1.272519\n","Epoch: 25/50... Steps: 3360... Loss: 1.1694... val_loss:  1.267509\n","Epoch: 25/50... Steps: 3370... Loss: 1.1589... val_loss:  1.265060\n","Epoch: 25/50... Steps: 3380... Loss: 1.1633... val_loss:  1.266828\n","Epoch: 25/50... Steps: 3390... Loss: 1.1502... val_loss:  1.269428\n","Epoch: 25/50... Steps: 3400... Loss: 1.1649... val_loss:  1.269601\n","Epoch: 25/50... Steps: 3410... Loss: 1.1781... val_loss:  1.268346\n","Epoch: 25/50... Steps: 3420... Loss: 1.1568... val_loss:  1.269263\n","Epoch: 25/50... Steps: 3430... Loss: 1.1656... val_loss:  1.274780\n","Epoch: 25/50... Steps: 3440... Loss: 1.1421... val_loss:  1.272548\n","Epoch: 25/50... Steps: 3450... Loss: 1.1608... val_loss:  1.268496\n","Epoch: 25/50... Steps: 3460... Loss: 1.1732... val_loss:  1.269095\n","Epoch: 25/50... Steps: 3470... Loss: 1.1702... val_loss:  1.263612\n","Epoch: 26/50... Steps: 3480... Loss: 1.1771... val_loss:  1.272884\n","Epoch: 26/50... Steps: 3490... Loss: 1.1646... val_loss:  1.265833\n","Epoch: 26/50... Steps: 3500... Loss: 1.1456... val_loss:  1.263242\n","Epoch: 26/50... Steps: 3510... Loss: 1.1674... val_loss:  1.266444\n","Epoch: 26/50... Steps: 3520... Loss: 1.1533... val_loss:  1.268578\n","Epoch: 26/50... Steps: 3530... Loss: 1.1580... val_loss:  1.265755\n","Epoch: 26/50... Steps: 3540... Loss: 1.1813... val_loss:  1.261883\n","Epoch: 26/50... Steps: 3550... Loss: 1.1503... val_loss:  1.266484\n","Epoch: 26/50... Steps: 3560... Loss: 1.1556... val_loss:  1.265749\n","Epoch: 26/50... Steps: 3570... Loss: 1.1492... val_loss:  1.260727\n","Epoch: 26/50... Steps: 3580... Loss: 1.1669... val_loss:  1.264279\n","Epoch: 26/50... Steps: 3590... Loss: 1.1471... val_loss:  1.259951\n","Epoch: 26/50... Steps: 3600... Loss: 1.1112... val_loss:  1.267113\n","Epoch: 26/50... Steps: 3610... Loss: 1.1641... val_loss:  1.262264\n","Epoch: 27/50... Steps: 3620... Loss: 1.1472... val_loss:  1.267997\n","Epoch: 27/50... Steps: 3630... Loss: 1.1537... val_loss:  1.266594\n","Epoch: 27/50... Steps: 3640... Loss: 1.1423... val_loss:  1.263200\n","Epoch: 27/50... Steps: 3650... Loss: 1.1507... val_loss:  1.262117\n","Epoch: 27/50... Steps: 3660... Loss: 1.1440... val_loss:  1.262775\n","Epoch: 27/50... Steps: 3670... Loss: 1.1559... val_loss:  1.260448\n","Epoch: 27/50... Steps: 3680... Loss: 1.1756... val_loss:  1.260211\n","Epoch: 27/50... Steps: 3690... Loss: 1.1323... val_loss:  1.259275\n","Epoch: 27/50... Steps: 3700... Loss: 1.1474... val_loss:  1.258745\n","Epoch: 27/50... Steps: 3710... Loss: 1.1383... val_loss:  1.255473\n","Epoch: 27/50... Steps: 3720... Loss: 1.1448... val_loss:  1.259426\n","Epoch: 27/50... Steps: 3730... Loss: 1.1625... val_loss:  1.258965\n","Epoch: 27/50... Steps: 3740... Loss: 1.1555... val_loss:  1.260357\n","Epoch: 27/50... Steps: 3750... Loss: 1.1579... val_loss:  1.257822\n","Epoch: 28/50... Steps: 3760... Loss: 1.1356... val_loss:  1.252560\n","Epoch: 28/50... Steps: 3770... Loss: 1.1403... val_loss:  1.254371\n","Epoch: 28/50... Steps: 3780... Loss: 1.1386... val_loss:  1.252639\n","Epoch: 28/50... Steps: 3790... Loss: 1.1570... val_loss:  1.258392\n","Epoch: 28/50... Steps: 3800... Loss: 1.1615... val_loss:  1.257549\n","Epoch: 28/50... Steps: 3810... Loss: 1.1467... val_loss:  1.254837\n","Epoch: 28/50... Steps: 3820... Loss: 1.1498... val_loss:  1.254449\n","Epoch: 28/50... Steps: 3830... Loss: 1.1377... val_loss:  1.257603\n","Epoch: 28/50... Steps: 3840... Loss: 1.1359... val_loss:  1.255803\n","Epoch: 28/50... Steps: 3850... Loss: 1.1511... val_loss:  1.256294\n","Epoch: 28/50... Steps: 3860... Loss: 1.1402... val_loss:  1.258691\n","Epoch: 28/50... Steps: 3870... Loss: 1.1299... val_loss:  1.250734\n","Epoch: 28/50... Steps: 3880... Loss: 1.1218... val_loss:  1.253082\n","Epoch: 28/50... Steps: 3890... Loss: 1.1313... val_loss:  1.252903\n","Epoch: 29/50... Steps: 3900... Loss: 1.1322... val_loss:  1.254341\n","Epoch: 29/50... Steps: 3910... Loss: 1.1491... val_loss:  1.252580\n","Epoch: 29/50... Steps: 3920... Loss: 1.1548... val_loss:  1.249469\n","Epoch: 29/50... Steps: 3930... Loss: 1.1624... val_loss:  1.250537\n","Epoch: 29/50... Steps: 3940... Loss: 1.1291... val_loss:  1.248846\n","Epoch: 29/50... Steps: 3950... Loss: 1.1426... val_loss:  1.246803\n","Epoch: 29/50... Steps: 3960... Loss: 1.1269... val_loss:  1.248627\n","Epoch: 29/50... Steps: 3970... Loss: 1.1643... val_loss:  1.252836\n","Epoch: 29/50... Steps: 3980... Loss: 1.1277... val_loss:  1.255908\n","Epoch: 29/50... Steps: 3990... Loss: 1.1217... val_loss:  1.251018\n","Epoch: 29/50... Steps: 4000... Loss: 1.1216... val_loss:  1.255534\n","Epoch: 29/50... Steps: 4010... Loss: 1.1197... val_loss:  1.248638\n","Epoch: 29/50... Steps: 4020... Loss: 1.1208... val_loss:  1.247333\n","Epoch: 29/50... Steps: 4030... Loss: 1.1361... val_loss:  1.243631\n","Epoch: 30/50... Steps: 4040... Loss: 1.1381... val_loss:  1.250299\n","Epoch: 30/50... Steps: 4050... Loss: 1.1394... val_loss:  1.247312\n","Epoch: 30/50... Steps: 4060... Loss: 1.1552... val_loss:  1.246970\n","Epoch: 30/50... Steps: 4070... Loss: 1.1475... val_loss:  1.253268\n","Epoch: 30/50... Steps: 4080... Loss: 1.1436... val_loss:  1.245536\n","Epoch: 30/50... Steps: 4090... Loss: 1.1483... val_loss:  1.247032\n","Epoch: 30/50... Steps: 4100... Loss: 1.1089... val_loss:  1.245967\n","Epoch: 30/50... Steps: 4110... Loss: 1.1157... val_loss:  1.242109\n","Epoch: 30/50... Steps: 4120... Loss: 1.1075... val_loss:  1.247021\n","Epoch: 30/50... Steps: 4130... Loss: 1.1050... val_loss:  1.247366\n","Epoch: 30/50... Steps: 4140... Loss: 1.1251... val_loss:  1.249286\n","Epoch: 30/50... Steps: 4150... Loss: 1.1178... val_loss:  1.245296\n","Epoch: 30/50... Steps: 4160... Loss: 1.1475... val_loss:  1.244709\n","Epoch: 30/50... Steps: 4170... Loss: 1.1708... val_loss:  1.241903\n","Epoch: 31/50... Steps: 4180... Loss: 1.1470... val_loss:  1.247768\n","Epoch: 31/50... Steps: 4190... Loss: 1.1592... val_loss:  1.245967\n","Epoch: 31/50... Steps: 4200... Loss: 1.1596... val_loss:  1.244972\n","Epoch: 31/50... Steps: 4210... Loss: 1.1367... val_loss:  1.244449\n","Epoch: 31/50... Steps: 4220... Loss: 1.1514... val_loss:  1.239874\n","Epoch: 31/50... Steps: 4230... Loss: 1.0979... val_loss:  1.240114\n","Epoch: 31/50... Steps: 4240... Loss: 1.1187... val_loss:  1.245518\n","Epoch: 31/50... Steps: 4250... Loss: 1.0983... val_loss:  1.252078\n","Epoch: 31/50... Steps: 4260... Loss: 1.1234... val_loss:  1.247530\n","Epoch: 31/50... Steps: 4270... Loss: 1.1246... val_loss:  1.245107\n","Epoch: 31/50... Steps: 4280... Loss: 1.1086... val_loss:  1.246129\n","Epoch: 31/50... Steps: 4290... Loss: 1.0975... val_loss:  1.243904\n","Epoch: 31/50... Steps: 4300... Loss: 1.1222... val_loss:  1.240074\n","Epoch: 32/50... Steps: 4310... Loss: 1.2033... val_loss:  1.240225\n","Epoch: 32/50... Steps: 4320... Loss: 1.1292... val_loss:  1.243935\n","Epoch: 32/50... Steps: 4330... Loss: 1.1361... val_loss:  1.244635\n","Epoch: 32/50... Steps: 4340... Loss: 1.1457... val_loss:  1.243519\n","Epoch: 32/50... Steps: 4350... Loss: 1.1081... val_loss:  1.245139\n","Epoch: 32/50... Steps: 4360... Loss: 1.1054... val_loss:  1.240753\n","Epoch: 32/50... Steps: 4370... Loss: 1.0991... val_loss:  1.239692\n","Epoch: 32/50... Steps: 4380... Loss: 1.1071... val_loss:  1.240234\n","Epoch: 32/50... Steps: 4390... Loss: 1.1050... val_loss:  1.238391\n","Epoch: 32/50... Steps: 4400... Loss: 1.1030... val_loss:  1.241231\n","Epoch: 32/50... Steps: 4410... Loss: 1.1189... val_loss:  1.245254\n","Epoch: 32/50... Steps: 4420... Loss: 1.1010... val_loss:  1.245727\n","Epoch: 32/50... Steps: 4430... Loss: 1.0867... val_loss:  1.244922\n","Epoch: 32/50... Steps: 4440... Loss: 1.1426... val_loss:  1.241628\n","Epoch: 33/50... Steps: 4450... Loss: 1.1119... val_loss:  1.240411\n","Epoch: 33/50... Steps: 4460... Loss: 1.1154... val_loss:  1.240017\n","Epoch: 33/50... Steps: 4470... Loss: 1.1118... val_loss:  1.240682\n","Epoch: 33/50... Steps: 4480... Loss: 1.1037... val_loss:  1.239113\n","Epoch: 33/50... Steps: 4490... Loss: 1.0901... val_loss:  1.241363\n","Epoch: 33/50... Steps: 4500... Loss: 1.0972... val_loss:  1.237205\n","Epoch: 33/50... Steps: 4510... Loss: 1.1251... val_loss:  1.236414\n","Epoch: 33/50... Steps: 4520... Loss: 1.0948... val_loss:  1.239302\n","Epoch: 33/50... Steps: 4530... Loss: 1.0723... val_loss:  1.242337\n","Epoch: 33/50... Steps: 4540... Loss: 1.0970... val_loss:  1.240624\n","Epoch: 33/50... Steps: 4550... Loss: 1.1260... val_loss:  1.242484\n","Epoch: 33/50... Steps: 4560... Loss: 1.1024... val_loss:  1.247207\n","Epoch: 33/50... Steps: 4570... Loss: 1.0886... val_loss:  1.252005\n","Epoch: 33/50... Steps: 4580... Loss: 1.1219... val_loss:  1.236525\n","Epoch: 34/50... Steps: 4590... Loss: 1.1183... val_loss:  1.240865\n","Epoch: 34/50... Steps: 4600... Loss: 1.1026... val_loss:  1.240870\n","Epoch: 34/50... Steps: 4610... Loss: 1.1213... val_loss:  1.233994\n","Epoch: 34/50... Steps: 4620... Loss: 1.0763... val_loss:  1.237457\n","Epoch: 34/50... Steps: 4630... Loss: 1.0634... val_loss:  1.238292\n","Epoch: 34/50... Steps: 4640... Loss: 1.1158... val_loss:  1.236369\n","Epoch: 34/50... Steps: 4650... Loss: 1.1263... val_loss:  1.235438\n","Epoch: 34/50... Steps: 4660... Loss: 1.1178... val_loss:  1.236616\n","Epoch: 34/50... Steps: 4670... Loss: 1.1265... val_loss:  1.236497\n","Epoch: 34/50... Steps: 4680... Loss: 1.1050... val_loss:  1.236651\n","Epoch: 34/50... Steps: 4690... Loss: 1.1034... val_loss:  1.242935\n","Epoch: 34/50... Steps: 4700... Loss: 1.0949... val_loss:  1.243065\n","Epoch: 34/50... Steps: 4710... Loss: 1.0683... val_loss:  1.246214\n","Epoch: 34/50... Steps: 4720... Loss: 1.1202... val_loss:  1.240714\n","Epoch: 35/50... Steps: 4730... Loss: 1.1020... val_loss:  1.240320\n","Epoch: 35/50... Steps: 4740... Loss: 1.1094... val_loss:  1.241846\n","Epoch: 35/50... Steps: 4750... Loss: 1.0911... val_loss:  1.237195\n","Epoch: 35/50... Steps: 4760... Loss: 1.0940... val_loss:  1.238781\n","Epoch: 35/50... Steps: 4770... Loss: 1.0931... val_loss:  1.237271\n","Epoch: 35/50... Steps: 4780... Loss: 1.0821... val_loss:  1.237831\n","Epoch: 35/50... Steps: 4790... Loss: 1.1033... val_loss:  1.239227\n","Epoch: 35/50... Steps: 4800... Loss: 1.1093... val_loss:  1.238882\n","Epoch: 35/50... Steps: 4810... Loss: 1.0901... val_loss:  1.237886\n","Epoch: 35/50... Steps: 4820... Loss: 1.0943... val_loss:  1.239276\n","Epoch: 35/50... Steps: 4830... Loss: 1.0844... val_loss:  1.245399\n","Epoch: 35/50... Steps: 4840... Loss: 1.0939... val_loss:  1.239770\n","Epoch: 35/50... Steps: 4850... Loss: 1.1052... val_loss:  1.244655\n","Epoch: 35/50... Steps: 4860... Loss: 1.1040... val_loss:  1.241077\n","Epoch: 36/50... Steps: 4870... Loss: 1.1103... val_loss:  1.242196\n","Epoch: 36/50... Steps: 4880... Loss: 1.0948... val_loss:  1.240863\n","Epoch: 36/50... Steps: 4890... Loss: 1.0879... val_loss:  1.239064\n","Epoch: 36/50... Steps: 4900... Loss: 1.1054... val_loss:  1.237700\n","Epoch: 36/50... Steps: 4910... Loss: 1.0751... val_loss:  1.240700\n","Epoch: 36/50... Steps: 4920... Loss: 1.0996... val_loss:  1.240173\n","Epoch: 36/50... Steps: 4930... Loss: 1.1226... val_loss:  1.236838\n","Epoch: 36/50... Steps: 4940... Loss: 1.0951... val_loss:  1.239324\n","Epoch: 36/50... Steps: 4950... Loss: 1.0867... val_loss:  1.240208\n","Epoch: 36/50... Steps: 4960... Loss: 1.0877... val_loss:  1.235478\n","Epoch: 36/50... Steps: 4970... Loss: 1.1040... val_loss:  1.241498\n","Epoch: 36/50... Steps: 4980... Loss: 1.0812... val_loss:  1.239677\n","Epoch: 36/50... Steps: 4990... Loss: 1.0498... val_loss:  1.244661\n","Epoch: 36/50... Steps: 5000... Loss: 1.1035... val_loss:  1.238568\n","Epoch: 37/50... Steps: 5010... Loss: 1.0834... val_loss:  1.242817\n","Epoch: 37/50... Steps: 5020... Loss: 1.0821... val_loss:  1.246028\n","Epoch: 37/50... Steps: 5030... Loss: 1.0864... val_loss:  1.240559\n","Epoch: 37/50... Steps: 5040... Loss: 1.0868... val_loss:  1.239449\n","Epoch: 37/50... Steps: 5050... Loss: 1.0856... val_loss:  1.239838\n","Epoch: 37/50... Steps: 5060... Loss: 1.0957... val_loss:  1.242090\n","Epoch: 37/50... Steps: 5070... Loss: 1.1043... val_loss:  1.242151\n","Epoch: 37/50... Steps: 5080... Loss: 1.0727... val_loss:  1.238424\n","Epoch: 37/50... Steps: 5090... Loss: 1.0955... val_loss:  1.240364\n","Epoch: 37/50... Steps: 5100... Loss: 1.0785... val_loss:  1.237359\n","Epoch: 37/50... Steps: 5110... Loss: 1.0769... val_loss:  1.239763\n","Epoch: 37/50... Steps: 5120... Loss: 1.1020... val_loss:  1.241404\n","Epoch: 37/50... Steps: 5130... Loss: 1.0957... val_loss:  1.242121\n","Epoch: 37/50... Steps: 5140... Loss: 1.0962... val_loss:  1.241396\n","Epoch: 38/50... Steps: 5150... Loss: 1.0800... val_loss:  1.245687\n","Epoch: 38/50... Steps: 5160... Loss: 1.0778... val_loss:  1.244099\n","Epoch: 38/50... Steps: 5170... Loss: 1.0821... val_loss:  1.238547\n","Epoch: 38/50... Steps: 5180... Loss: 1.1106... val_loss:  1.241472\n","Epoch: 38/50... Steps: 5190... Loss: 1.1010... val_loss:  1.240464\n","Epoch: 38/50... Steps: 5200... Loss: 1.0822... val_loss:  1.240870\n","Epoch: 38/50... Steps: 5210... Loss: 1.1008... val_loss:  1.243760\n","Epoch: 38/50... Steps: 5220... Loss: 1.0808... val_loss:  1.241865\n","Epoch: 38/50... Steps: 5230... Loss: 1.0772... val_loss:  1.239038\n","Epoch: 38/50... Steps: 5240... Loss: 1.0848... val_loss:  1.239709\n","Epoch: 38/50... Steps: 5250... Loss: 1.0821... val_loss:  1.242766\n","Epoch: 38/50... Steps: 5260... Loss: 1.0783... val_loss:  1.236990\n","Epoch: 38/50... Steps: 5270... Loss: 1.0674... val_loss:  1.241203\n","Epoch: 38/50... Steps: 5280... Loss: 1.0816... val_loss:  1.236583\n","Epoch: 39/50... Steps: 5290... Loss: 1.0777... val_loss:  1.240771\n","Epoch: 39/50... Steps: 5300... Loss: 1.0933... val_loss:  1.241626\n","Epoch: 39/50... Steps: 5310... Loss: 1.1001... val_loss:  1.241600\n","Epoch: 39/50... Steps: 5320... Loss: 1.1028... val_loss:  1.240684\n","Epoch: 39/50... Steps: 5330... Loss: 1.0755... val_loss:  1.240628\n","Epoch: 39/50... Steps: 5340... Loss: 1.0933... val_loss:  1.246316\n","Epoch: 39/50... Steps: 5350... Loss: 1.0701... val_loss:  1.242322\n","Epoch: 39/50... Steps: 5360... Loss: 1.1119... val_loss:  1.240997\n","Epoch: 39/50... Steps: 5370... Loss: 1.0769... val_loss:  1.244623\n","Epoch: 39/50... Steps: 5380... Loss: 1.0762... val_loss:  1.240732\n","Epoch: 39/50... Steps: 5390... Loss: 1.0730... val_loss:  1.242816\n","Epoch: 39/50... Steps: 5400... Loss: 1.0733... val_loss:  1.238743\n","Epoch: 39/50... Steps: 5410... Loss: 1.0718... val_loss:  1.241952\n","Epoch: 39/50... Steps: 5420... Loss: 1.0877... val_loss:  1.240335\n","Epoch: 40/50... Steps: 5430... Loss: 1.0861... val_loss:  1.239131\n","Epoch: 40/50... Steps: 5440... Loss: 1.0905... val_loss:  1.242747\n","Epoch: 40/50... Steps: 5450... Loss: 1.1040... val_loss:  1.243641\n","Epoch: 40/50... Steps: 5460... Loss: 1.0843... val_loss:  1.247741\n","Epoch: 40/50... Steps: 5470... Loss: 1.0758... val_loss:  1.241226\n","Epoch: 40/50... Steps: 5480... Loss: 1.0810... val_loss:  1.245035\n","Epoch: 40/50... Steps: 5490... Loss: 1.0633... val_loss:  1.242864\n","Epoch: 40/50... Steps: 5500... Loss: 1.0654... val_loss:  1.244137\n","Epoch: 40/50... Steps: 5510... Loss: 1.0627... val_loss:  1.245238\n","Epoch: 40/50... Steps: 5520... Loss: 1.0619... val_loss:  1.242356\n","Epoch: 40/50... Steps: 5530... Loss: 1.0648... val_loss:  1.242758\n","Epoch: 40/50... Steps: 5540... Loss: 1.0660... val_loss:  1.238835\n","Epoch: 40/50... Steps: 5550... Loss: 1.0882... val_loss:  1.243725\n","Epoch: 40/50... Steps: 5560... Loss: 1.1348... val_loss:  1.239896\n","Epoch: 41/50... Steps: 5570... Loss: 1.1064... val_loss:  1.236123\n","Epoch: 41/50... Steps: 5580... Loss: 1.0931... val_loss:  1.240533\n","Epoch: 41/50... Steps: 5590... Loss: 1.1124... val_loss:  1.240829\n","Epoch: 41/50... Steps: 5600... Loss: 1.0780... val_loss:  1.240830\n","Epoch: 41/50... Steps: 5610... Loss: 1.0841... val_loss:  1.234616\n","Epoch: 41/50... Steps: 5620... Loss: 1.0402... val_loss:  1.237714\n","Epoch: 41/50... Steps: 5630... Loss: 1.0705... val_loss:  1.236148\n","Epoch: 41/50... Steps: 5640... Loss: 1.0515... val_loss:  1.241425\n","Epoch: 41/50... Steps: 5650... Loss: 1.0813... val_loss:  1.239747\n","Epoch: 41/50... Steps: 5660... Loss: 1.0812... val_loss:  1.240319\n","Epoch: 41/50... Steps: 5670... Loss: 1.0662... val_loss:  1.239360\n","Epoch: 41/50... Steps: 5680... Loss: 1.0553... val_loss:  1.235694\n","Epoch: 41/50... Steps: 5690... Loss: 1.0835... val_loss:  1.243283\n","Epoch: 42/50... Steps: 5700... Loss: 1.1676... val_loss:  1.238036\n","Epoch: 42/50... Steps: 5710... Loss: 1.0913... val_loss:  1.232918\n","Epoch: 42/50... Steps: 5720... Loss: 1.0825... val_loss:  1.237227\n","Epoch: 42/50... Steps: 5730... Loss: 1.0999... val_loss:  1.239505\n","Epoch: 42/50... Steps: 5740... Loss: 1.0640... val_loss:  1.240591\n","Epoch: 42/50... Steps: 5750... Loss: 1.0462... val_loss:  1.237628\n","Epoch: 42/50... Steps: 5760... Loss: 1.0463... val_loss:  1.238676\n","Epoch: 42/50... Steps: 5770... Loss: 1.0632... val_loss:  1.242671\n","Epoch: 42/50... Steps: 5780... Loss: 1.0564... val_loss:  1.242685\n","Epoch: 42/50... Steps: 5790... Loss: 1.0668... val_loss:  1.242939\n","Epoch: 42/50... Steps: 5800... Loss: 1.0819... val_loss:  1.239514\n","Epoch: 42/50... Steps: 5810... Loss: 1.0670... val_loss:  1.239611\n","Epoch: 42/50... Steps: 5820... Loss: 1.0537... val_loss:  1.243184\n","Epoch: 42/50... Steps: 5830... Loss: 1.0932... val_loss:  1.237714\n","Epoch: 43/50... Steps: 5840... Loss: 1.0585... val_loss:  1.232876\n","Epoch: 43/50... Steps: 5850... Loss: 1.0758... val_loss:  1.237055\n","Epoch: 43/50... Steps: 5860... Loss: 1.0722... val_loss:  1.239580\n","Epoch: 43/50... Steps: 5870... Loss: 1.0597... val_loss:  1.243827\n","Epoch: 43/50... Steps: 5880... Loss: 1.0451... val_loss:  1.238763\n","Epoch: 43/50... Steps: 5890... Loss: 1.0525... val_loss:  1.233733\n","Epoch: 43/50... Steps: 5900... Loss: 1.0865... val_loss:  1.237267\n","Epoch: 43/50... Steps: 5910... Loss: 1.0575... val_loss:  1.238019\n","Epoch: 43/50... Steps: 5920... Loss: 1.0331... val_loss:  1.238874\n","Epoch: 43/50... Steps: 5930... Loss: 1.0574... val_loss:  1.240064\n","Epoch: 43/50... Steps: 5940... Loss: 1.0808... val_loss:  1.236157\n","Epoch: 43/50... Steps: 5950... Loss: 1.0616... val_loss:  1.243248\n","Epoch: 43/50... Steps: 5960... Loss: 1.0465... val_loss:  1.236895\n","Epoch: 43/50... Steps: 5970... Loss: 1.0709... val_loss:  1.234554\n","Epoch: 44/50... Steps: 5980... Loss: 1.0827... val_loss:  1.235345\n","Epoch: 44/50... Steps: 5990... Loss: 1.0637... val_loss:  1.236588\n","Epoch: 44/50... Steps: 6000... Loss: 1.0863... val_loss:  1.232980\n","Epoch: 44/50... Steps: 6010... Loss: 1.0411... val_loss:  1.237650\n","Epoch: 44/50... Steps: 6020... Loss: 1.0220... val_loss:  1.238099\n","Epoch: 44/50... Steps: 6030... Loss: 1.0671... val_loss:  1.232753\n","Epoch: 44/50... Steps: 6040... Loss: 1.0812... val_loss:  1.233733\n","Epoch: 44/50... Steps: 6050... Loss: 1.0730... val_loss:  1.237771\n","Epoch: 44/50... Steps: 6060... Loss: 1.0866... val_loss:  1.242144\n","Epoch: 44/50... Steps: 6070... Loss: 1.0652... val_loss:  1.236949\n","Epoch: 44/50... Steps: 6080... Loss: 1.0549... val_loss:  1.238549\n","Epoch: 44/50... Steps: 6090... Loss: 1.0498... val_loss:  1.235959\n","Epoch: 44/50... Steps: 6100... Loss: 1.0360... val_loss:  1.241233\n","Epoch: 44/50... Steps: 6110... Loss: 1.0834... val_loss:  1.235540\n","Epoch: 45/50... Steps: 6120... Loss: 1.0588... val_loss:  1.241318\n","Epoch: 45/50... Steps: 6130... Loss: 1.0719... val_loss:  1.235887\n","Epoch: 45/50... Steps: 6140... Loss: 1.0555... val_loss:  1.234952\n","Epoch: 45/50... Steps: 6150... Loss: 1.0507... val_loss:  1.239265\n","Epoch: 45/50... Steps: 6160... Loss: 1.0590... val_loss:  1.240088\n","Epoch: 45/50... Steps: 6170... Loss: 1.0416... val_loss:  1.236122\n","Epoch: 45/50... Steps: 6180... Loss: 1.0591... val_loss:  1.239979\n","Epoch: 45/50... Steps: 6190... Loss: 1.0692... val_loss:  1.238072\n","Epoch: 45/50... Steps: 6200... Loss: 1.0583... val_loss:  1.242617\n","Epoch: 45/50... Steps: 6210... Loss: 1.0699... val_loss:  1.236826\n","Epoch: 45/50... Steps: 6220... Loss: 1.0477... val_loss:  1.237674\n","Epoch: 45/50... Steps: 6230... Loss: 1.0587... val_loss:  1.238530\n","Epoch: 45/50... Steps: 6240... Loss: 1.0677... val_loss:  1.246289\n","Epoch: 45/50... Steps: 6250... Loss: 1.0604... val_loss:  1.239721\n","Epoch: 46/50... Steps: 6260... Loss: 1.0679... val_loss:  1.242398\n","Epoch: 46/50... Steps: 6270... Loss: 1.0622... val_loss:  1.241738\n","Epoch: 46/50... Steps: 6280... Loss: 1.0570... val_loss:  1.234626\n","Epoch: 46/50... Steps: 6290... Loss: 1.0666... val_loss:  1.241992\n","Epoch: 46/50... Steps: 6300... Loss: 1.0446... val_loss:  1.241359\n","Epoch: 46/50... Steps: 6310... Loss: 1.0699... val_loss:  1.242036\n","Epoch: 46/50... Steps: 6320... Loss: 1.0729... val_loss:  1.245194\n","Epoch: 46/50... Steps: 6330... Loss: 1.0496... val_loss:  1.241007\n","Epoch: 46/50... Steps: 6340... Loss: 1.0492... val_loss:  1.244296\n","Epoch: 46/50... Steps: 6350... Loss: 1.0439... val_loss:  1.238299\n","Epoch: 46/50... Steps: 6360... Loss: 1.0684... val_loss:  1.244499\n","Epoch: 46/50... Steps: 6370... Loss: 1.0503... val_loss:  1.239952\n","Epoch: 46/50... Steps: 6380... Loss: 1.0154... val_loss:  1.245921\n","Epoch: 46/50... Steps: 6390... Loss: 1.0669... val_loss:  1.239005\n","Epoch: 47/50... Steps: 6400... Loss: 1.0500... val_loss:  1.241344\n","Epoch: 47/50... Steps: 6410... Loss: 1.0575... val_loss:  1.241923\n","Epoch: 47/50... Steps: 6420... Loss: 1.0433... val_loss:  1.235899\n","Epoch: 47/50... Steps: 6430... Loss: 1.0490... val_loss:  1.247128\n","Epoch: 47/50... Steps: 6440... Loss: 1.0522... val_loss:  1.244203\n","Epoch: 47/50... Steps: 6450... Loss: 1.0660... val_loss:  1.244198\n","Epoch: 47/50... Steps: 6460... Loss: 1.0706... val_loss:  1.242281\n","Epoch: 47/50... Steps: 6470... Loss: 1.0390... val_loss:  1.243112\n","Epoch: 47/50... Steps: 6480... Loss: 1.0608... val_loss:  1.241550\n","Epoch: 47/50... Steps: 6490... Loss: 1.0445... val_loss:  1.238465\n","Epoch: 47/50... Steps: 6500... Loss: 1.0345... val_loss:  1.241710\n","Epoch: 47/50... Steps: 6510... Loss: 1.0553... val_loss:  1.242343\n","Epoch: 47/50... Steps: 6520... Loss: 1.0527... val_loss:  1.246192\n","Epoch: 47/50... Steps: 6530... Loss: 1.0496... val_loss:  1.239787\n","Epoch: 48/50... Steps: 6540... Loss: 1.0469... val_loss:  1.240558\n","Epoch: 48/50... Steps: 6550... Loss: 1.0450... val_loss:  1.246041\n","Epoch: 48/50... Steps: 6560... Loss: 1.0476... val_loss:  1.238361\n","Epoch: 48/50... Steps: 6570... Loss: 1.0574... val_loss:  1.241992\n","Epoch: 48/50... Steps: 6580... Loss: 1.0689... val_loss:  1.244119\n","Epoch: 48/50... Steps: 6590... Loss: 1.0532... val_loss:  1.245708\n","Epoch: 48/50... Steps: 6600... Loss: 1.0640... val_loss:  1.243388\n","Epoch: 48/50... Steps: 6610... Loss: 1.0406... val_loss:  1.241917\n","Epoch: 48/50... Steps: 6620... Loss: 1.0403... val_loss:  1.242651\n","Epoch: 48/50... Steps: 6630... Loss: 1.0587... val_loss:  1.241848\n","Epoch: 48/50... Steps: 6640... Loss: 1.0431... val_loss:  1.241048\n","Epoch: 48/50... Steps: 6650... Loss: 1.0447... val_loss:  1.240123\n","Epoch: 48/50... Steps: 6660... Loss: 1.0336... val_loss:  1.246725\n","Epoch: 48/50... Steps: 6670... Loss: 1.0455... val_loss:  1.244086\n","Epoch: 49/50... Steps: 6680... Loss: 1.0433... val_loss:  1.237755\n","Epoch: 49/50... Steps: 6690... Loss: 1.0555... val_loss:  1.249781\n","Epoch: 49/50... Steps: 6700... Loss: 1.0623... val_loss:  1.239610\n","Epoch: 49/50... Steps: 6710... Loss: 1.0744... val_loss:  1.245190\n","Epoch: 49/50... Steps: 6720... Loss: 1.0394... val_loss:  1.248974\n","Epoch: 49/50... Steps: 6730... Loss: 1.0538... val_loss:  1.247547\n","Epoch: 49/50... Steps: 6740... Loss: 1.0509... val_loss:  1.242814\n","Epoch: 49/50... Steps: 6750... Loss: 1.0679... val_loss:  1.247223\n","Epoch: 49/50... Steps: 6760... Loss: 1.0462... val_loss:  1.244176\n","Epoch: 49/50... Steps: 6770... Loss: 1.0426... val_loss:  1.244166\n","Epoch: 49/50... Steps: 6780... Loss: 1.0433... val_loss:  1.246079\n","Epoch: 49/50... Steps: 6790... Loss: 1.0345... val_loss:  1.240324\n","Epoch: 49/50... Steps: 6800... Loss: 1.0327... val_loss:  1.248433\n","Epoch: 49/50... Steps: 6810... Loss: 1.0556... val_loss:  1.250558\n","Epoch: 50/50... Steps: 6820... Loss: 1.0443... val_loss:  1.241418\n","Epoch: 50/50... Steps: 6830... Loss: 1.0601... val_loss:  1.250828\n","Epoch: 50/50... Steps: 6840... Loss: 1.0671... val_loss:  1.240129\n","Epoch: 50/50... Steps: 6850... Loss: 1.0507... val_loss:  1.242708\n","Epoch: 50/50... Steps: 6860... Loss: 1.0386... val_loss:  1.251280\n","Epoch: 50/50... Steps: 6870... Loss: 1.0442... val_loss:  1.250086\n","Epoch: 50/50... Steps: 6880... Loss: 1.0292... val_loss:  1.244372\n","Epoch: 50/50... Steps: 6890... Loss: 1.0272... val_loss:  1.247048\n","Epoch: 50/50... Steps: 6900... Loss: 1.0217... val_loss:  1.249576\n","Epoch: 50/50... Steps: 6910... Loss: 1.0197... val_loss:  1.244098\n","Epoch: 50/50... Steps: 6920... Loss: 1.0318... val_loss:  1.243082\n","Epoch: 50/50... Steps: 6930... Loss: 1.0314... val_loss:  1.242697\n","Epoch: 50/50... Steps: 6940... Loss: 1.0654... val_loss:  1.249858\n","Epoch: 50/50... Steps: 6950... Loss: 1.0952... val_loss:  1.245257\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Oc1MGqgBrgpz","executionInfo":{"status":"ok","timestamp":1630321388395,"user_tz":-60,"elapsed":385,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#Saving the model\n","net=RNN(chars,n_hidden, n_layers, drop_prob, lr)\n","ckpt_path='/content/drive/MyDrive/Colab Notebooks/Char_RNN'\n","name='RNN_20_epoch.net'\n","epoch=20\n","path= os.path.join(ckpt_path, 'net_{}_{}.pth'.format(name, epoch))\n","torch.save(net.state_dict(), path)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"RDdWnpEAGS5T","executionInfo":{"status":"ok","timestamp":1630321372170,"user_tz":-60,"elapsed":365,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#saving the model\n","\n","model_name='rnn_20_epoch.net'\n","\n","checkpoint={'n_hidden': net.n_hidden,\n","            'n_layers': net.n_layers,\n","            'state_dict': net.state_dict(),\n","            'tokens': net.chars}\n","\n","with open(model_name, 'wb') as f:\n","    torch.save(checkpoint, f)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwiFTmc-GS5T","executionInfo":{"status":"ok","timestamp":1630321448890,"user_tz":-60,"elapsed":359,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["def predict(net, char, h=None, top_k=None):\n","    x=np.array([[net.char2int[char]]])\n","    y=one_hot_encode(x, len(net.chars))\n","    inputs= torch.from_numpy(x)\n","    \n","    if (gpu_train):\n","        inputs=inputs.cuda()\n","        \n","        h=tuple ([each.data for each in h])\n","        output, h= net(inputs, h)\n","\n","        p=F.softmax(output, dim=1).data\n","        \n","        #get top K characters\n","        if (gpu_train):\n","          p=p.cpu()\n","\n","        \n","        if top_k is None:\n","            top_ch= np.arange(len(net.chars))\n","            \n","        else:\n","            p, top_ch=p.topk(top_k)\n","            top_ch= top_ch.numpy().squeeze()\n","        p=p.numpy().squeeze()\n","        \n","        char=np.random.choice(top_ch, p=p/p.sum())\n","        \n","        return net.int2char[char], h\n","        "],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bsaHvFaGS5T","executionInfo":{"status":"ok","timestamp":1630321452477,"user_tz":-60,"elapsed":408,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#Printing and generating text by the neural network\n","\n","def sample(net, size, prime='The', top_k=None):\n","    if(gpu_train):\n","        net.cuda()\n","    else:\n","        net.cpu()\n","    net.eval()\n","    \n","    chars=[ch for ch in prime]\n","    \n","    h=net.init_hidden(1)\n","    for ch in prime:\n","        char, h= predict(net, ch,h,top_k=top_k)\n","    chars.append(char)\n","    \n","    for ii in range(size):\n","        char, h= predict(net, chars[-1], h, top_k=top_k)\n","        chars.append(char)\n","    return ''.join(chars)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"IZckyDvIxoev","executionInfo":{"status":"error","timestamp":1630324604896,"user_tz":-60,"elapsed":688,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"1e5a918c-ba6a-4dea-d76e-6955821b50c1"},"source":["#Loading the model and using it to generate a sample text\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/Char_RNN/net_RNN_20_epoch.net_20.pth', 'rb') as f:\n","  model=torch.load(f)\n","\n","loaded_model=RNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'],drop_prob=0.5, lr=0.001 )\n","loaded_model.load_state_dict(checkpoint['state_dict'])\n","\n","print(sample(net, 1000, prime='Levine Said', top_k=5))"],"execution_count":41,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-fc81901dce26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Levine Said'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-34-64758a93bf14>\u001b[0m in \u001b[0;36msample\u001b[0;34m(net, size, prime, top_k)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-61388b3103e4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(net, char, h, top_k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-de0128c5eddd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     31\u001b[0m      \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                  \u001b[0;31m#generate output and new hidden unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mr_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#send output via drop out layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    618\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                            ):\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    622\u001b[0m                                'Expected hidden[0] size {}, got {}')\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    201\u001b[0m             raise RuntimeError(\n\u001b[1;32m    202\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 203\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise RuntimeError(\n","\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"]}]},{"cell_type":"code","metadata":{"id":"oimVhXBLGS5U","colab":{"base_uri":"https://localhost:8080/","height":303},"executionInfo":{"status":"error","timestamp":1630324614111,"user_tz":-60,"elapsed":404,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"41728971-7109-4f30-f37d-215668d93126"},"source":["print(sample(net,1000,prime='Anna', top_k= 5))"],"execution_count":42,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-045667f853f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Anna'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-34-64758a93bf14>\u001b[0m in \u001b[0;36msample\u001b[0;34m(net, size, prime, top_k)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-61388b3103e4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(net, char, h, top_k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-de0128c5eddd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     31\u001b[0m      \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                  \u001b[0;31m#generate output and new hidden unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mr_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#send output via drop out layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    618\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                            ):\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    622\u001b[0m                                'Expected hidden[0] size {}, got {}')\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    201\u001b[0m             raise RuntimeError(\n\u001b[1;32m    202\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 203\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise RuntimeError(\n","\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"]}]},{"cell_type":"code","metadata":{"id":"t_ee9O5XW4cv"},"source":[""],"execution_count":null,"outputs":[]}]}